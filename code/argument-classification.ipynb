{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "\n",
    "#import pandas, xgboost, numpy, textblob, string\n",
    "# from keras.preprocessing import text, sequence\n",
    "# from keras import layers, models, optimizers\n",
    "import gensim\n",
    "import umap\n",
    "import umap.plot\n",
    "from yellowbrick.text import TSNEVisualizer\n",
    "import re, string,os\n",
    "from glob import glob as gb\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta, date\n",
    "from collections import OrderedDict\n",
    "import subprocess\n",
    "import fuzzywuzzy\n",
    "from fuzzywuzzy import process,fuzz\n",
    "from utils.functions import *\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "from nltk.corpus import stopwords\n",
    "from classification import *\n",
    "\n",
    "# First test: https://www.analyticsvidhya.com/blog/2018/04/a-comprehensive-guide-to-understand-and-implement-text-classification-in-python/\n",
    "# Word2vec: http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import annotated data\n",
    "annotations = pd.read_csv('~/Documents/GitHub/CrisisBureaucracy/data/classifier/annotated-arguments-bureaucracy.csv')\n",
    "annotations = annotations[[\"id\",\"label\",\"text\"]]\n",
    "annotations['metadata'] = ''\n",
    "\n",
    "refdata = pd.read_csv('~/Documents/GitHub/CrisisBureaucracy/data/classifier/training_data_full.csv',sep='\\t')\n",
    "refdata['id-ann'] = [x + 594 for x in refdata.index]\n",
    "\n",
    "for c,i in enumerate(annotations['id']):\n",
    "    annotations['metadata'][c] = str(refdata[refdata['id-ann'] == i].reset_index(drop=True)['id'][0])\n",
    "\n",
    "\n",
    "stops = stopwords.words('english') + [\"hon\",\"member\",\"right\",\"friend\",\"mr\",'hon.','make','say','great']\n",
    "annotations['text'] = utils.preprocess_(annotations['text'])\n",
    "annotations = annotations.drop('id',axis=1).reset_index(drop=True)\n",
    "\n",
    "labels = {1:\"neutral\",2:\"inefficient\",3:\"powerful/large\",4:\"centralization\",5:\"freedom\",6:\"expensive\",7:\"anti-democratic\"}\n",
    "annotations['label'] = annotations['label'].astype(str)\n",
    "\n",
    "#annotations = annotations[(annotations['label'] != '1') & (annotations['label'] != '3')].reset_index(drop=True)\n",
    "\n",
    "# model = gensim.models.KeyedVectors.load_word2vec_format('/media/ruben/Elements/GoogleNews-vectors-negative300-SLIM.bin/GoogleNews-vectors-negative300-SLIM.bin',binary=True)\n",
    "# w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect Category TF-IDF terms\n",
    "tfidfo, docterms = tfidf.get_docterms(annotations,\"text\")\n",
    "tt = tfidf.get_topterms(tfidfo,docterms,annotations,'label')\n",
    "#tt.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(annotations['text'], annotations['label'])\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(annotations['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    \n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SVM, N-Gram Vectors:  0.45023696682464454\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "#accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "#print(\"NB, N-Gram Vectors: \", accuracy)\n",
    "\n",
    "# SVM on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "print(\"SVM, N-Gram Vectors: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(annotations['text'], annotations['label'],train_size=0.5)\n",
    "\n",
    "# label encode the target variable \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)\n",
    "\n",
    "# ngram level tf-idf \n",
    "tfidf_vect_ngram = TfidfEmbeddingVectorizer(word2vec=w2v) #TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(annotations['text'],annotations['label'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "\n",
    "#TfidfEmbeddingVectorizer()"
   ]
  }
 ]
}