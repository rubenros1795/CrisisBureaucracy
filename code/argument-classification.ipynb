{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV  # or GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import re, string,os\n",
    "from glob import glob as gb\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from utils.functions import *\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# https://towardsdatascience.com/journey-to-the-center-of-multi-label-classification-384c40229bff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from TagTog format\n",
    "translator = {\"m_10\":\"cost\",\"m_11\":\"accountability\",\"m_12\":\"freedom\",\"m_13\":\"inefficiency\",\"m_14\":\"irrationality\",\n",
    "              \"m_15\":\"centralisation\",\"m_16\":\"power\",\"m_8\":\"neutral\",\"m_9\":\"size\"}\n",
    "\n",
    "def load_full(id_):\n",
    "    f = pd.read_csv(f'/media/ruben/Elements/PhD/data/hansard/lemmatized_pm/uk.proc.d.{id_[:10]}.txt',sep='\\t')\n",
    "    return [list(f[f['id'] == 'uk.proc.d.' + id_]['text'])[0],list(f[f['id'] == 'uk.proc.d.' + id_]['text_lemmatized'])[0]]\n",
    "\n",
    "def combine(fn):\n",
    "    with open(fn,'r') as f:\n",
    "        c = json.load(f)\n",
    "    labels = [translator[x] for x in list(c['metas'].keys())]\n",
    "    txtn = \"/home/ruben/Documents/GitHub/CrisisBureaucracy/data/classifier/annotation-round-2/annotation-texts/\" + fn[130:].replace('.ann.json','').replace('_','-')\n",
    "    with open(ltf_[txtn],'r') as f:\n",
    "        t = f.read()\n",
    "    return [os.path.split(ltf_[txtn])[-1].replace('.txt',''),t,labels]\n",
    "\n",
    "lf = gb('/home/ruben/Documents/GitHub/CrisisBureaucracy/data/classifier/annotation-round-2/annotation-results/*')\n",
    "ltf = gb('/home/ruben/Documents/GitHub/CrisisBureaucracy/data/classifier/annotation-round-2/annotation-texts/*')\n",
    "ltf_ = {k.replace('_','-'):k for k in ltf}\n",
    "\n",
    "df = [combine(x) for x in lf]\n",
    "df = pd.DataFrame(df,columns=['id','text','label'])\n",
    "or_texts = [load_full(\"-\".join(x.split('-')[1:-1])) for x in df['id']]\n",
    "df['original_par'] = [x[0] for x in or_texts]\n",
    "df = df[['id','original_par','label']]\n",
    "df.columns = \"id text label\".split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 750/750 [00:13<00:00, 54.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "stops = stopwords.words('english') + \"hon member friend gentleman gentlemen speaker right\".split(' ')\n",
    "df['text'] = [\" \".join([w.text for w in nlp(t) if w.pos_ in [\"ADJ\",\"NOUN\",\"VERB\"] and str(w.text) not in stops]) for t in tqdm(df['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model train function\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross validation on separate categories \n",
      " ---------\n",
      "irrationality \t 0.98 0.98 0.98 0.98 0.98\n",
      "cost \t 0.86 0.86 0.86 0.86 0.86\n",
      "accountability \t 0.93 0.93 0.92 0.92 0.92\n",
      "inefficiency \t 0.84 0.84 0.84 0.84 0.83\n",
      "freedom \t 0.91 0.91 0.90 0.90 0.90\n",
      "size \t 0.68 0.72 0.7 0.72 0.70\n",
      "centralisation \t 0.95 0.95 0.95 0.94 0.94\n",
      "neutral \t 0.78 0.78 0.78 0.77 0.77\n",
      "power \t 0.94 0.94 0.94 0.94 0.93\n"
     ]
    }
   ],
   "source": [
    "print('Cross validation on separate categories \\n ---------')\n",
    "for cat in set([item for sublist in list(df['label']) for item in sublist]):\n",
    "    dfc = df.copy()\n",
    "    dfc['label'] = [cat if cat in x else \"other\" for x in dfc['label']]\n",
    "\n",
    "    # Vectorize\n",
    "    tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=8000)\n",
    "    tfidf_vect.fit(dfc['text'])\n",
    "\n",
    "    # Use Cross-Validation\n",
    "    clf = make_pipeline(tfidf_vect, naive_bayes.MultinomialNB())\n",
    "    scores = cross_validate(clf, dfc['text'], dfc['label'], scoring=['accuracy'], cv=5, return_train_score=False)\n",
    "    print(cat,'\\t',\" \".join([str(x)[:4] for x in scores['test_accuracy']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(set([item for sublist in list(df['label']) for item in sublist]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = df[['id','text','label']]\n",
    "for c in categories:\n",
    "    dfr[c] = [1 if c in i else 0 for i in dfr['label']]\n",
    "dfr =dfr.drop(['id','label'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(dfr, random_state=42, test_size=0.33, shuffle=True)\n",
    "X_train = train.text\n",
    "X_test = test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "irrationality:\t 0.98\n",
      "cost:\t 0.871\n",
      "accountability:\t 0.927\n",
      "inefficiency:\t 0.823\n",
      "freedom:\t 0.891\n",
      "size:\t 0.738\n",
      "centralisation:\t 0.927\n",
      "neutral:\t 0.774\n",
      "power:\t 0.948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Define a pipeline combining a text feature extractor with multi lable classifier\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "for category in categories:\n",
    "    # train the model using X_dtm & y\n",
    "    NB_pipeline.fit(X_train, train[category])\n",
    "    # compute the testing accuracy\n",
    "    prediction = NB_pipeline.predict(X_test)\n",
    "    print(f'{category}:\\t {round(accuracy_score(test[category], prediction),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy =  0.11290322580645161\n"
     ]
    }
   ],
   "source": [
    "# initialize binary relevance multi-label classifier\n",
    "# with a gaussian naive bayes base classifier\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=10000)\n",
    "tfidf_vect.fit(dfr['text'])\n",
    "train,test = train_test_split(dfr, random_state=42, test_size=0.33, shuffle=True)\n",
    "\n",
    "xtrain_tfidf =  tfidf_vect.transform(train.text)\n",
    "xvalid_tfidf =  tfidf_vect.transform(test.text)\n",
    "\n",
    "classifier = BinaryRelevance(GaussianNB())\n",
    "# train\n",
    "classifier.fit(xtrain_tfidf, train.drop(labels = ['text'], axis=1))\n",
    "# predict\n",
    "predictions = classifier.predict(xvalid_tfidf)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(test.drop(labels = ['text'], axis=1),predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy =  0.27419354838709675\n\n\n"
     ]
    }
   ],
   "source": [
    "# using classifier chains\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# initialize classifier chains multi-label classifier\n",
    "classifier = ClassifierChain(LogisticRegression())\n",
    "# Training logistic regression model on train data\n",
    "classifier.fit(xtrain_tfidf, train.drop(labels = ['text'], axis=1))\n",
    "# predict\n",
    "predictions = classifier.predict(xvalid_tfidf)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(test.drop(labels = ['text'], axis=1),predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy =  0.2056451612903226\n\n\n"
     ]
    }
   ],
   "source": [
    "# using Label Powerset\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "# initialize label powerset multi-label classifier\n",
    "classifier = LabelPowerset(LogisticRegression())\n",
    "# train\n",
    "# Training logistic regression model on train data\n",
    "classifier.fit(xtrain_tfidf, train.drop(labels = ['text'], axis=1))\n",
    "# predict\n",
    "predictions = classifier.predict(xvalid_tfidf)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(test.drop(labels = ['text'], axis=1),predictions))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy =  0.13709677419354838\n\n\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "classifier_new = MLkNN(k=10)\n",
    "# Note that this classifier can throw up errors when handling sparse matrices.\n",
    "x_train = lil_matrix(xtrain_tfidf).toarray()\n",
    "y_train = lil_matrix(train.drop(labels = ['text'], axis=1)).toarray()\n",
    "x_test = lil_matrix(xvalid_tfidf).toarray()\n",
    "# train\n",
    "classifier_new.fit(x_train, y_train)\n",
    "# predict\n",
    "predictions_new = classifier_new.predict(x_test)\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(test.drop(labels = ['text'], axis=1),predictions_new))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}