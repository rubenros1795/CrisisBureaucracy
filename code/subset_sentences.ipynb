{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from glob import glob as gb\n",
    "from random import sample\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import pandas as pd\n",
    "import math\n",
    "import spacy\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "import string \n",
    "from utils.functions import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/media/ruben/Elements/PhD/data/hansard/lemmatized_pm\"\n",
    "list_files = gb(data_path + \"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subset paragraphs that mention bureaucracy with grep, into one dataframe\n",
    "path = \"/media/ruben/Elements/PhD/data/hansard/lemmatized_pm/*\"\n",
    "grp = \"egrep -iE '\" + \"|\".join([\"bureaucracy\"]) + \"' \" + path\n",
    "output = subprocess.check_output(grp,shell=True).decode('utf-8')\n",
    "output = [l.split('\\t') for l in output.split('\\n')]\n",
    "data = pd.DataFrame(output)\n",
    "data.columns = \"id text topic_id topic_name scene_id scene_name speech_id speech_member_ref speech_member_ref_name speech_member_party num_char text_lemmatized\".split(' ')\n",
    "data = data[data['text'].astype(str) != \"text\"].reset_index(drop=True)\n",
    "data = data.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1457 [0, 2]\n",
      "2019 [0, 2]\n",
      "2302 [0, 2]\n",
      "2789 [0, 2]\n"
     ]
    }
   ],
   "source": [
    "## Extract 3-sentence windows, add speaker-id and id \n",
    "#  I take one sentence left, and one sentence right to the sentence containing the keyword.\n",
    "\n",
    "def sentences2windows(sentences,ind_):\n",
    "    left = ind_- 1\n",
    "    right = ind_ + 2\n",
    "    if left < 0:\n",
    "        left = 0\n",
    "    if right > len(sentences):\n",
    "        right = len(sentences)\n",
    "    return \" \".join(sentences[left:right])\n",
    "\n",
    "windows = []\n",
    "\n",
    "for c,t in enumerate(data['text_lemmatized']):\n",
    "    sentences = nltk.sent_tokenize(t)\n",
    "    indices = [c for c,s in enumerate(sentences) if \"bureaucracy\" in set(s.split(' '))]\n",
    "    \n",
    "    if len(indices) == 0:\n",
    "        continue \n",
    "    elif len(indices) == 1:\n",
    "        windows.append([data['id'][c],data['speech_member_ref'][c],data['speech_member_party'][c],sentences2windows(sentences,indices[0])])\n",
    "    elif len(indices) > 1:\n",
    "        if abs(indices[0]-indices[1]) == 1:\n",
    "            windows.append([data['id'][c],data['speech_member_ref'][c],data['speech_member_party'][c],sentences2windows(sentences,indices[0])])\n",
    "        else:\n",
    "            print(c,indices)\n",
    "df = pd.DataFrame(windows,columns=[\"id\",\"member-ref\",\"member-party\",\"window\"])\n",
    "df['id'] = [df['member-ref'][c][5:] + '-' + os.path.split(x)[-1].split(':')[1][10:] + \"-\" + df['member-party'][c].replace(' ','_').lower() for c,x in enumerate(df['id'])]\n",
    "df = df[[\"id\",\"window\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/media/ruben/OSDisk/Users/ruben.ros/Documents/GitHub/CrisisBureaucracy/data/bureaucracy-sentences-full.tsv',sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/media/ruben/OSDisk/Users/ruben.ros/Documents/GitHub/CrisisBureaucracy/data/bureaucracy-sentences-sample-before1970.html','w') as f:\n",
    "    ht = df_sample.to_html(bold_rows=True,table_id=\"table-content\",index_names=False,index=False)\n",
    "    ht = ht.replace(' bureaucracy ',' <b>bureaucracy</b> ')\n",
    "    ht = '<html>' + '<link rel=\"stylesheet\" href=\"css/style.css\" /> <link href=\"https://fonts.googleapis.com/css2?family=Crimson+Text&family=Lato&family=Raleway&display=swap\" rel=\"stylesheet\">' + ht + '</html>'\n",
    "    f.write(ht)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df)) < 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = df[~msk].reset_index(drop=True)\n",
    "other_data = df[msk].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_full = training_data\n",
    "training_data_doccano = pd.DataFrame(training_data_full.iloc[:,1])\n",
    "other_data_full = other_data\n",
    "other_data_doccano = pd.DataFrame(other_data_full.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/media/ruben/OSDisk/Users/ruben.ros/Documents/GitHub/CrisisBureaucracy/data/classifier'\n",
    "training_data_full.to_csv(os.path.join(base_path,\"training_data_full.csv\"),index=False,sep='\\t')\n",
    "training_data_doccano.to_csv(os.path.join(base_path,\"training_data_doccano.txt\"),index=False,sep='\\t',header=False)\n",
    "other_data_full.to_csv(os.path.join(base_path,\"other_data_full.csv\"),index=False,sep='\\t')\n",
    "other_data_doccano.to_csv(os.path.join(base_path,\"other_data_doccano.txt\"),index=False,sep='\\t',header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subset for Round 2 - TagToc\n",
    "\n",
    "df = pd.read_csv('~/Documents/GitHub/CrisisBureaucracy/data/classifier/bureaucracy-sentences-full.tsv',sep='\\t').dropna()\n",
    "sample_ = df.sample(750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r in sample_.iterrows():\n",
    "\n",
    "    with open('/home/ruben/Documents/GitHub/CrisisBureaucracy/data/classifier/annotation-round-2/' + r['id'] + \".txt\",'w') as f:\n",
    "         f.write(r['window'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}